{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df=pd.read_pickle('./data/train_df.pickle')\n",
    "test_df=pd.read_pickle('./data/test_df.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 3600000/3600000 [00:25<00:00, 142602.81it/s]\n"
     ]
    }
   ],
   "source": [
    "comment_lengths=list()\n",
    "comment_list=train_df['text'].tolist()\n",
    "for i in tqdm(range(len(comment_list))):\n",
    "    comment=comment_list[i]\n",
    "    temp1=len(comment.split())\n",
    "    comment_lengths.append(temp1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 400000/400000 [00:02<00:00, 140844.08it/s]\n"
     ]
    }
   ],
   "source": [
    "comment_lengths_test=list()\n",
    "comment_list_test=test_df['text'].tolist()\n",
    "for i in tqdm(range(len(comment_list_test))):\n",
    "    comment=comment_list_test[i]\n",
    "    temp1=len(comment.split())\n",
    "    comment_lengths_test.append(temp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#word2vec_simple_from_tf=np.load('./data/word2vec_simple.npz')\n",
    "word2vec_simple_from_tf=np.load('./data/word2vec_1M_FT.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings=word2vec_simple_from_tf['arr_0']\n",
    "word_dict=word2vec_simple_from_tf['arr_1'].tolist()\n",
    "reverse_dict=word2vec_simple_from_tf['arr_2'].tolist()\n",
    "embedding_size=embeddings.shape[1]\n",
    "comment_vector_size=embedding_size*max(comment_lengths)\n",
    "num_comments=len(comment_list)\n",
    "train_labels=(train_df['label']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 3600000/3600000 [03:17<00:00, 18239.20it/s]\n"
     ]
    }
   ],
   "source": [
    "comments_with_word_indices=np.zeros(shape=(num_comments,max(comment_lengths)),dtype='int32')\n",
    "for i in tqdm(range(len(comment_list))):\n",
    "    comment=comment_list[i]\n",
    "    comment_words=comment.split()\n",
    "    for j,word in enumerate(comment_words):\n",
    "        comments_with_word_indices[i,j]=word_dict.get(word,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_batch(batch_size):\n",
    "    batch_inputs=np.zeros((batch_size,comment_vector_size))\n",
    "    batch_labels=np.ndarray(shape=(batch_size,1),dtype=np.float64)\n",
    "    batch_indices=random.sample(range(num_comments),batch_size)\n",
    "    for i in range(batch_size):\n",
    "        batch_labels[i]=train_labels[batch_indices[i]]-1\n",
    "        comment_with_word_indices=comments_with_word_indices[batch_indices[i],:]\n",
    "        for j,word_index in enumerate(comment_with_word_indices):\n",
    "            if word_index==0:\n",
    "                word_vector=np.zeros(embedding_size)\n",
    "            else:\n",
    "                word_vector=embeddings[word_index,:]\n",
    "            batch_inputs[i,j*embedding_size:(j+1)*embedding_size]=word_vector\n",
    "    return batch_inputs, batch_labels, batch_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38076\n",
      "139\n",
      "[ 1.]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#sanity check\n",
    "a=generate_batch(2) \n",
    "b=a[0][0]\n",
    "index1=a[2][0]\n",
    "c=[o for o in b if o!=0]\n",
    "print(len(c))\n",
    "print(len(comment_list[index1].split()))\n",
    "label1=a[1][0]\n",
    "print(label1)\n",
    "print(train_df['label'][index1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./ckpt/LR_word2vec_1M_FT.ckpt\n",
      "initialized\n",
      "Step: 1000 ,Average loss in  1000 steps:  0.366232348491  ,batch time:  65.36192917823792  ,Run time:  22.020044326782227\n",
      "Step: 2000 ,Average loss in  1000 steps:  0.361558607394  ,batch time:  65.96992564201355  ,Run time:  21.88983392715454\n",
      "Step: 3000 ,Average loss in  1000 steps:  0.372287089663  ,batch time:  65.52271389961243  ,Run time:  22.043843507766724\n",
      "Step: 4000 ,Average loss in  1000 steps:  0.368673681609  ,batch time:  65.43512105941772  ,Run time:  22.08903741836548\n",
      "Step: 5000 ,Average loss in  1000 steps:  0.361723040212  ,batch time:  65.74311780929565  ,Run time:  21.900840520858765\n",
      "Step: 6000 ,Average loss in  1000 steps:  0.361959425925  ,batch time:  65.63792252540588  ,Run time:  21.87463617324829\n",
      "Step: 7000 ,Average loss in  1000 steps:  0.363373453813  ,batch time:  65.25931644439697  ,Run time:  22.16904354095459\n",
      "Step: 8000 ,Average loss in  1000 steps:  0.360461611408  ,batch time:  65.42251539230347  ,Run time:  21.8454430103302\n",
      "Step: 9000 ,Average loss in  1000 steps:  0.363732137316  ,batch time:  65.47591781616211  ,Run time:  22.022041082382202\n",
      "Step: 10000 ,Average loss in  1000 steps:  0.365524747517  ,batch time:  65.35451602935791  ,Run time:  22.0064435005188\n",
      "Step: 11000 ,Average loss in  1000 steps:  0.364309538043  ,batch time:  65.47832155227661  ,Run time:  21.91483783721924\n",
      "Step: 12000 ,Average loss in  1000 steps:  0.363571273765  ,batch time:  65.68512034416199  ,Run time:  21.82423996925354\n",
      "Step: 13000 ,Average loss in  1000 steps:  0.36799231661  ,batch time:  65.55232834815979  ,Run time:  22.025036096572876\n",
      "Step: 14000 ,Average loss in  1000 steps:  0.360001395312  ,batch time:  65.44812631607056  ,Run time:  22.041637897491455\n",
      "Step: 15000 ,Average loss in  1000 steps:  0.365870492237  ,batch time:  65.36892414093018  ,Run time:  21.90023708343506\n",
      "Step: 16000 ,Average loss in  1000 steps:  0.360741212988  ,batch time:  65.37552571296692  ,Run time:  22.016435623168945\n",
      "Step: 17000 ,Average loss in  1000 steps:  0.365989430199  ,batch time:  65.0473198890686  ,Run time:  22.0718412399292\n",
      "Step: 18000 ,Average loss in  1000 steps:  0.366549469793  ,batch time:  65.56771802902222  ,Run time:  21.927443265914917\n",
      "Step: 19000 ,Average loss in  1000 steps:  0.361246593848  ,batch time:  65.49311852455139  ,Run time:  21.959643840789795\n",
      "Step: 20000 ,Average loss in  1000 steps:  0.364644252562  ,batch time:  65.44672584533691  ,Run time:  21.955636978149414\n",
      "Step: 21000 ,Average loss in  1000 steps:  0.365044079102  ,batch time:  65.2757203578949  ,Run time:  22.179041862487793\n",
      "Step: 22000 ,Average loss in  1000 steps:  0.360370269199  ,batch time:  65.59931898117065  ,Run time:  21.991843223571777\n",
      "Step: 23000 ,Average loss in  1000 steps:  0.364735849527  ,batch time:  65.01951885223389  ,Run time:  22.14884352684021\n",
      "Step: 24000 ,Average loss in  1000 steps:  0.365336152132  ,batch time:  65.73872709274292  ,Run time:  22.054235458374023\n",
      "Step: 25000 ,Average loss in  1000 steps:  0.366870431045  ,batch time:  65.94592308998108  ,Run time:  21.980241060256958\n",
      "Step: 26000 ,Average loss in  1000 steps:  0.362530604344  ,batch time:  65.62892127037048  ,Run time:  21.998842239379883\n",
      "Step: 27000 ,Average loss in  1000 steps:  0.359519317594  ,batch time:  65.71392369270325  ,Run time:  22.015440464019775\n",
      "Step: 28000 ,Average loss in  1000 steps:  0.364441122166  ,batch time:  65.96712493896484  ,Run time:  21.7824387550354\n",
      "Step: 29000 ,Average loss in  1000 steps:  0.360754312344  ,batch time:  65.7731192111969  ,Run time:  21.951447248458862\n",
      "Step: 30000 ,Average loss in  1000 steps:  0.357833279604  ,batch time:  65.48551940917969  ,Run time:  21.88644528388977\n",
      "Step: 31000 ,Average loss in  1000 steps:  0.364095558261  ,batch time:  66.02791976928711  ,Run time:  21.937845945358276\n",
      "Step: 32000 ,Average loss in  1000 steps:  0.360517553909  ,batch time:  65.44672131538391  ,Run time:  22.00424337387085\n",
      "Step: 33000 ,Average loss in  1000 steps:  0.361117924254  ,batch time:  65.56092190742493  ,Run time:  21.96644353866577\n",
      "Step: 34000 ,Average loss in  1000 steps:  0.365469479841  ,batch time:  66.19332480430603  ,Run time:  22.0228431224823\n",
      "Step: 35000 ,Average loss in  1000 steps:  0.35662831542  ,batch time:  65.93012475967407  ,Run time:  21.865243434906006\n",
      "Step: 36000 ,Average loss in  1000 steps:  0.365760850709  ,batch time:  65.92812967300415  ,Run time:  21.987637281417847\n",
      "Step: 37000 ,Average loss in  1000 steps:  0.362104223482  ,batch time:  65.70112228393555  ,Run time:  21.895044803619385\n",
      "Step: 38000 ,Average loss in  1000 steps:  0.364496302985  ,batch time:  65.98392963409424  ,Run time:  21.903637409210205\n",
      "Step: 39000 ,Average loss in  1000 steps:  0.361513810222  ,batch time:  65.82133030891418  ,Run time:  22.06263780593872\n",
      "Step: 40000 ,Average loss in  1000 steps:  0.362379704768  ,batch time:  65.75032424926758  ,Run time:  22.03944420814514\n",
      "Step: 41000 ,Average loss in  1000 steps:  0.361163137915  ,batch time:  65.56972599029541  ,Run time:  21.871440887451172\n",
      "Step: 42000 ,Average loss in  1000 steps:  0.35564580748  ,batch time:  65.69853806495667  ,Run time:  21.934829235076904\n",
      "Step: 43000 ,Average loss in  1000 steps:  0.357173019877  ,batch time:  65.70592498779297  ,Run time:  21.977643489837646\n",
      "Step: 44000 ,Average loss in  1000 steps:  0.360498288406  ,batch time:  65.59433054924011  ,Run time:  21.99563765525818\n",
      "Step: 45000 ,Average loss in  1000 steps:  0.356930478616  ,batch time:  65.29372906684875  ,Run time:  22.074239492416382\n",
      "Step: 46000 ,Average loss in  1000 steps:  0.354524021232  ,batch time:  65.3135232925415  ,Run time:  22.03844428062439\n",
      "Step: 47000 ,Average loss in  1000 steps:  0.359965764005  ,batch time:  65.62933850288391  ,Run time:  22.10543131828308\n",
      "Step: 48000 ,Average loss in  1000 steps:  0.363909282172  ,batch time:  65.50171852111816  ,Run time:  22.0660502910614\n",
      "Step: 49000 ,Average loss in  1000 steps:  0.362055553544  ,batch time:  65.7295253276825  ,Run time:  21.824044466018677\n",
      "Step: 50000 ,Average loss in  1000 steps:  0.357054111531  ,batch time:  65.21732664108276  ,Run time:  22.081642389297485\n",
      "Step: 51000 ,Average loss in  1000 steps:  0.360846217049  ,batch time:  65.42712831497192  ,Run time:  22.119841814041138\n",
      "Step: 52000 ,Average loss in  1000 steps:  0.359305702206  ,batch time:  65.95152997970581  ,Run time:  21.94383931159973\n",
      "Step: 53000 ,Average loss in  1000 steps:  0.35728241052  ,batch time:  65.77032446861267  ,Run time:  21.830045223236084\n",
      "Step: 54000 ,Average loss in  1000 steps:  0.362054348376  ,batch time:  65.87793278694153  ,Run time:  21.793639421463013\n",
      "Step: 55000 ,Average loss in  1000 steps:  0.361689055702  ,batch time:  65.66073441505432  ,Run time:  21.998039484024048\n",
      "Step: 56000 ,Average loss in  1000 steps:  0.36384474459  ,batch time:  66.0019359588623  ,Run time:  22.118239164352417\n",
      "Step: 57000 ,Average loss in  1000 steps:  0.353224661286  ,batch time:  65.25992393493652  ,Run time:  22.125847339630127\n",
      "Step: 58000 ,Average loss in  1000 steps:  0.360335785715  ,batch time:  65.77493214607239  ,Run time:  21.859038591384888\n",
      "Step: 59000 ,Average loss in  1000 steps:  0.360504734262  ,batch time:  65.82972598075867  ,Run time:  21.91544508934021\n",
      "Step: 60000 ,Average loss in  1000 steps:  0.357525316128  ,batch time:  65.86132454872131  ,Run time:  21.88984704017639\n",
      "Step: 61000 ,Average loss in  1000 steps:  0.358675709052  ,batch time:  65.85113334655762  ,Run time:  21.757439613342285\n",
      "Step: 62000 ,Average loss in  1000 steps:  0.361754463151  ,batch time:  65.73433136940002  ,Run time:  21.683441638946533\n",
      "Step: 63000 ,Average loss in  1000 steps:  0.354637322117  ,batch time:  65.57232666015625  ,Run time:  22.057446002960205\n",
      "Step: 64000 ,Average loss in  1000 steps:  0.358301453556  ,batch time:  65.64972805976868  ,Run time:  22.035844802856445\n",
      "Step: 65000 ,Average loss in  1000 steps:  0.359133768864  ,batch time:  66.04212927818298  ,Run time:  22.004645347595215\n",
      "Step: 66000 ,Average loss in  1000 steps:  0.355206970058  ,batch time:  66.06332993507385  ,Run time:  21.8954439163208\n",
      "Step: 67000 ,Average loss in  1000 steps:  0.358858495733  ,batch time:  65.86513233184814  ,Run time:  22.098240852355957\n",
      "Step: 68000 ,Average loss in  1000 steps:  0.355348811972  ,batch time:  65.46733951568604  ,Run time:  22.169635772705078\n",
      "Step: 69000 ,Average loss in  1000 steps:  0.358130944128  ,batch time:  65.53393411636353  ,Run time:  22.193645477294922\n",
      "Step: 70000 ,Average loss in  1000 steps:  0.358271535636  ,batch time:  65.78153467178345  ,Run time:  21.916038751602173\n",
      "Step: 71000 ,Average loss in  1000 steps:  0.357702644658  ,batch time:  65.82873439788818  ,Run time:  21.815840482711792\n",
      "Step: 72000 ,Average loss in  1000 steps:  0.361952802392  ,batch time:  65.93233275413513  ,Run time:  21.948242664337158\n",
      "Step: 73000 ,Average loss in  1000 steps:  0.364283614829  ,batch time:  65.86453557014465  ,Run time:  21.80523920059204\n",
      "Step: 74000 ,Average loss in  1000 steps:  0.358950560956  ,batch time:  65.45633792877197  ,Run time:  22.13623833656311\n",
      "Step: 75000 ,Average loss in  1000 steps:  0.356119444441  ,batch time:  65.71552515029907  ,Run time:  21.828250885009766\n",
      "Step: 76000 ,Average loss in  1000 steps:  0.361542047425  ,batch time:  66.01832866668701  ,Run time:  21.785449028015137\n",
      "Step: 77000 ,Average loss in  1000 steps:  0.355718671517  ,batch time:  66.3091368675232  ,Run time:  22.014442205429077\n",
      "Step: 78000 ,Average loss in  1000 steps:  0.357972251885  ,batch time:  65.80552458763123  ,Run time:  22.04265260696411\n",
      "Step: 79000 ,Average loss in  1000 steps:  0.352198067075  ,batch time:  66.35833621025085  ,Run time:  21.89164400100708\n",
      "Step: 80000 ,Average loss in  1000 steps:  0.359928237657  ,batch time:  65.81313800811768  ,Run time:  21.979238510131836\n",
      "Step: 81000 ,Average loss in  1000 steps:  0.361593499576  ,batch time:  65.73852729797363  ,Run time:  22.123650312423706\n",
      "Step: 82000 ,Average loss in  1000 steps:  0.360092227371  ,batch time:  66.37574577331543  ,Run time:  21.714035511016846\n",
      "Step: 83000 ,Average loss in  1000 steps:  0.360279307604  ,batch time:  65.81653881072998  ,Run time:  21.849839210510254\n",
      "Step: 84000 ,Average loss in  1000 steps:  0.365668664978  ,batch time:  65.71173453330994  ,Run time:  22.048243522644043\n",
      "Step: 85000 ,Average loss in  1000 steps:  0.359931384863  ,batch time:  66.16553926467896  ,Run time:  21.712639331817627\n",
      "Step: 86000 ,Average loss in  1000 steps:  0.361041110488  ,batch time:  65.77213382720947  ,Run time:  21.981843948364258\n",
      "Step: 87000 ,Average loss in  1000 steps:  0.355397390304  ,batch time:  65.66693830490112  ,Run time:  22.060839653015137\n",
      "Step: 88000 ,Average loss in  1000 steps:  0.358189428447  ,batch time:  65.74333357810974  ,Run time:  21.98684549331665\n",
      "Step: 89000 ,Average loss in  1000 steps:  0.363026380794  ,batch time:  65.53693294525146  ,Run time:  21.974246501922607\n",
      "Step: 90000 ,Average loss in  1000 steps:  0.356609710509  ,batch time:  66.12073945999146  ,Run time:  21.79163932800293\n",
      "Step: 91000 ,Average loss in  1000 steps:  0.352403919153  ,batch time:  65.68493390083313  ,Run time:  22.09604525566101\n",
      "Step: 92000 ,Average loss in  1000 steps:  0.365757493241  ,batch time:  65.26873254776001  ,Run time:  21.954046964645386\n",
      "Step: 93000 ,Average loss in  1000 steps:  0.358540462076  ,batch time:  65.39553475379944  ,Run time:  22.12564516067505\n",
      "Step: 94000 ,Average loss in  1000 steps:  0.357660778474  ,batch time:  65.5765426158905  ,Run time:  21.98223853111267\n",
      "Step: 95000 ,Average loss in  1000 steps:  0.363604116727  ,batch time:  65.67373466491699  ,Run time:  21.884645700454712\n",
      "Step: 96000 ,Average loss in  1000 steps:  0.360828653291  ,batch time:  65.71113324165344  ,Run time:  21.96204686164856\n",
      "Step: 97000 ,Average loss in  1000 steps:  0.360317012665  ,batch time:  65.59753489494324  ,Run time:  22.21124768257141\n",
      "Step: 98000 ,Average loss in  1000 steps:  0.355338277083  ,batch time:  65.6093385219574  ,Run time:  22.225242853164673\n",
      "Step: 99000 ,Average loss in  1000 steps:  0.361463772481  ,batch time:  65.82733511924744  ,Run time:  22.189847469329834\n",
      "Step: 100000 ,Average loss in  1000 steps:  0.357289614325  ,batch time:  66.01893424987793  ,Run time:  22.045047998428345\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "num_steps=100001\n",
    "batch_size=64\n",
    "graph=tf.Graph()\n",
    "batch_timing=0\n",
    "run_timing=0\n",
    "restore_sess=1\n",
    "#model_name=\"./ckpt/LR_word2vec.ckpt\"\n",
    "model_name=\"./ckpt/LR_word2vec_1M_FT.ckpt\"\n",
    "steps_display=min(int((num_steps-1)/20),1000)\n",
    "with graph.as_default():\n",
    "    inputs=tf.placeholder(tf.float64, shape=(batch_size,comment_vector_size))\n",
    "    labels=tf.placeholder(tf.float64, shape=(batch_size,1))\n",
    "    W = tf.Variable(tf.random_uniform([comment_vector_size, 1],-1.0,1.0,dtype=tf.float64))\n",
    "    b = tf.Variable(tf.ones([1],dtype=tf.float64))\n",
    "    y_pred=tf.matmul(inputs,W)+b\n",
    "    x_entropy=tf.nn.sigmoid_cross_entropy_with_logits(logits=y_pred,labels=labels)\n",
    "    loss =tf.reduce_mean(x_entropy)#+0.01*(tf.reduce_sum(tf.multiply(W,W))+tf.reduce_sum(tf.multiply(b,b)))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver=tf.train.Saver()    \n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    if restore_sess==1:\n",
    "        try:\n",
    "            saver.restore(sess,model_name)\n",
    "        except:\n",
    "            print('Unexpected Error: model cannot be restored')\n",
    "    print('initialized')\n",
    "    loss_val_sum=0\n",
    "    for i in range(num_steps):\n",
    "        time1=time.time()\n",
    "        batch_inputs,batch_labels,_=generate_batch(batch_size)\n",
    "        time2=time.time()\n",
    "        feed_dict={inputs:batch_inputs,labels:batch_labels}\n",
    "        _,loss_val,y_preds=sess.run([optimizer,loss,y_pred],feed_dict=feed_dict)\n",
    "        time3=time.time()\n",
    "        batch_timing += time2-time1\n",
    "        run_timing += time3-time2\n",
    "        loss_val_sum += loss_val\n",
    "        if i % steps_display==0 and i!=0:\n",
    "            print('Step:',i,',Average loss in ',steps_display, 'steps: ',loss_val_sum/steps_display,' ,batch time: ',batch_timing,' ,Run time: ',run_timing)\n",
    "            batch_timing=0\n",
    "            run_timing=0\n",
    "            loss_val_sum=0\n",
    "    Weights=W.eval()\n",
    "    bias=b.eval()\n",
    "    try:\n",
    "        saver.save(sess,model_name)\n",
    "        print('Model saved!')\n",
    "    except:\n",
    "        print('Model could not be saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 400000/400000 [02:40<00:00, 2486.74it/s]\n"
     ]
    }
   ],
   "source": [
    "num_comments_test=len(comment_list_test)\n",
    "test_labels=(test_df['label']-1).tolist()\n",
    "predictions=list()\n",
    "for i in tqdm(range(num_comments_test)):\n",
    "    comment_vector=np.zeros(shape=(comment_vector_size))\n",
    "    comment=comment_list_test[i]\n",
    "    comment_as_word=comment.split()\n",
    "    for j,word in enumerate(comment_as_word):\n",
    "        word_index=word_dict.get(word,0)\n",
    "        if word_index==0:\n",
    "            word_vector=np.zeros(embedding_size)\n",
    "        else:\n",
    "            word_vector=embeddings[word_index,:]\n",
    "        comment_vector[j*embedding_size:(j+1)*embedding_size]=word_vector\n",
    "    pred_logit=1 / (1 + np.exp(-(comment_vector.dot(Weights)+bias)))\n",
    "    if pred_logit>0.5:\n",
    "        pred=1\n",
    "    else:\n",
    "        pred=0\n",
    "    predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.42%, Precision: 78.94%, Recall: 84.90%, F1_score: 84.49%\n"
     ]
    }
   ],
   "source": [
    "y_true=test_labels\n",
    "y_pred=predictions\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "accuracy=accuracy_score(y_true,y_pred)\n",
    "precision=average_precision_score(y_true,y_pred)\n",
    "recall=recall_score(y_true,y_pred)\n",
    "f1_score=f1_score(y_true,y_pred)\n",
    "print('Accuracy: {:4.2f}%, Precision: {:4.2f}%, Recall: {:4.2f}%, F1_score: {:4.2f}%'.format(accuracy*100,precision*100,recall*100,f1_score*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
