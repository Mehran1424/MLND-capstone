{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df=pd.read_pickle('./data/train_df.pickle')\n",
    "#test_df=pd.read_pickle('./data/test_df.pickle')\n",
    "train_df=pd.read_pickle('./data/train_df_nopunc.pickle')\n",
    "test_df=pd.read_pickle('./data/test_df_nopunc.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 3600000/3600000 [00:27<00:00, 129831.57it/s]\n"
     ]
    }
   ],
   "source": [
    "comment_lengths=list()\n",
    "comment_list=train_df['text'].tolist()\n",
    "for i in tqdm(range(len(comment_list))):\n",
    "    comment=comment_list[i]\n",
    "    temp1=len(comment.split())\n",
    "    comment_lengths.append(temp1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 400000/400000 [00:03<00:00, 131758.07it/s]\n"
     ]
    }
   ],
   "source": [
    "comment_lengths_test=list()\n",
    "comment_list_test=test_df['text'].tolist()\n",
    "for i in tqdm(range(len(comment_list_test))):\n",
    "    comment=comment_list_test[i]\n",
    "    temp1=len(comment.split())\n",
    "    comment_lengths_test.append(temp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec_simple_from_tf=np.load('./data/word2vec_simple.npz')\n",
    "word2vec_simple_from_tf=np.load('./data/word2vec_1M_FT.npz')\n",
    "#word2vec_simple_from_tf=np.load('./data/word2vec_200k_amazon.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=word2vec_simple_from_tf['arr_0']\n",
    "word_dict=word2vec_simple_from_tf['arr_1'].tolist()\n",
    "reverse_dict=word2vec_simple_from_tf['arr_2'].tolist()\n",
    "embedding_size=embeddings.shape[1]\n",
    "comment_vector_size=embedding_size*max(comment_lengths)\n",
    "num_comments=len(comment_list)\n",
    "train_labels=(train_df['label']).tolist()\n",
    "n_embeddings,d_embeddings=embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 3600000/3600000 [03:18<00:00, 18165.69it/s]\n"
     ]
    }
   ],
   "source": [
    "comments_with_word_indices=np.zeros(shape=(num_comments,max(comment_lengths)),dtype='int32')\n",
    "for i in tqdm(range(len(comment_list))):\n",
    "    comment=comment_list[i]\n",
    "    comment_words=comment.split()\n",
    "    for j,word in enumerate(comment_words):\n",
    "        comments_with_word_indices[i,j]=word_dict.get(word,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch_size):\n",
    "    batch_indices=random.sample(range(num_comments),batch_size)\n",
    "    batch_inputs=comments_with_word_indices[batch_indices,:]\n",
    "    batch_labels=np.reshape(np.asarray([train_labels[idx] for idx in batch_indices])-1,(batch_size,1))\n",
    "    return batch_inputs, batch_labels, batch_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "40\n",
      "[0]\n",
      "1\n",
      "[  132  1093   619   104 39289  8780    65     5     2  2501    59   174\n",
      "    48     0   174    29   163    35   915   135   989   990     4     4\n",
      "  1147  2401     0    12   144   462  6577  2501     5    30    82   139\n",
      "    20  1598     2   316     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0]\n",
      " This price makes me cringe  Sure some of the movies were good  but $350 good  I think not  Not even special features and and whole extra  never-before-seen-alternate-series that s 8 alternate movies  of this would make it worth the money\n"
     ]
    }
   ],
   "source": [
    "#sanity check\n",
    "a=generate_batch(2) \n",
    "b=a[0][0]\n",
    "index1=a[2][0]\n",
    "c=[o for o in b if o!=0]\n",
    "print(len(c))\n",
    "print(len(comment_list[index1].split()))\n",
    "label1=a[1][0]\n",
    "print(label1)\n",
    "print(train_df['label'][index1])\n",
    "print(b)\n",
    "print(comment_list[a[2][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 400000/400000 [00:22<00:00, 17954.77it/s]\n"
     ]
    }
   ],
   "source": [
    "#creating test dataset\n",
    "num_comments_test=len(comment_list_test)\n",
    "test_labels=(test_df['label']-1).tolist()\n",
    "test_dataset=np.zeros((num_comments_test,max(comment_lengths)),dtype='int32')\n",
    "for i in tqdm(range(num_comments_test)):\n",
    "    comment=comment_list_test[i]\n",
    "    comment_as_word=comment.split()\n",
    "    for j,word in enumerate(comment_as_word):\n",
    "        word_index=word_dict.get(word,0)\n",
    "        test_dataset[i,j]=word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./ckpt/LR_word2vec_1M_FT_nopunc.ckpt\n",
      "initialized\n",
      "Step: 50 ,Average loss in 50 steps: 0.317 ,batch time: 0.02 ,Run time: 1.27 Batch Accuracy: 87.50 Valid accuracy: 88.40\n",
      "Step: 100 ,Average loss in 50 steps: 0.317 ,batch time: 0.00 ,Run time: 1.14 Batch Accuracy: 92.19 Valid accuracy: 89.80\n",
      "Step: 150 ,Average loss in 50 steps: 0.295 ,batch time: 0.02 ,Run time: 1.13 Batch Accuracy: 95.31 Valid accuracy: 86.80\n",
      "Step: 200 ,Average loss in 50 steps: 0.324 ,batch time: 0.02 ,Run time: 1.16 Batch Accuracy: 92.19 Valid accuracy: 85.20\n",
      "Step: 250 ,Average loss in 50 steps: 0.297 ,batch time: 0.00 ,Run time: 1.18 Batch Accuracy: 90.62 Valid accuracy: 85.80\n",
      "Step: 300 ,Average loss in 50 steps: 0.305 ,batch time: 0.00 ,Run time: 1.18 Batch Accuracy: 89.06 Valid accuracy: 88.40\n",
      "Step: 350 ,Average loss in 50 steps: 0.326 ,batch time: 0.02 ,Run time: 1.16 Batch Accuracy: 79.69 Valid accuracy: 85.00\n",
      "Step: 400 ,Average loss in 50 steps: 0.319 ,batch time: 0.00 ,Run time: 1.17 Batch Accuracy: 84.38 Valid accuracy: 86.60\n",
      "Step: 450 ,Average loss in 50 steps: 0.338 ,batch time: 0.00 ,Run time: 1.19 Batch Accuracy: 87.50 Valid accuracy: 86.60\n",
      "Step: 500 ,Average loss in 50 steps: 0.297 ,batch time: 0.05 ,Run time: 1.12 Batch Accuracy: 87.50 Valid accuracy: 86.20\n",
      "Step: 550 ,Average loss in 50 steps: 0.288 ,batch time: 0.00 ,Run time: 1.19 Batch Accuracy: 85.94 Valid accuracy: 89.40\n",
      "Step: 600 ,Average loss in 50 steps: 0.316 ,batch time: 0.02 ,Run time: 1.20 Batch Accuracy: 82.81 Valid accuracy: 87.00\n",
      "Step: 650 ,Average loss in 50 steps: 0.320 ,batch time: 0.05 ,Run time: 1.13 Batch Accuracy: 81.25 Valid accuracy: 87.40\n",
      "Step: 700 ,Average loss in 50 steps: 0.310 ,batch time: 0.03 ,Run time: 1.16 Batch Accuracy: 87.50 Valid accuracy: 85.60\n",
      "Step: 750 ,Average loss in 50 steps: 0.294 ,batch time: 0.00 ,Run time: 1.16 Batch Accuracy: 79.69 Valid accuracy: 86.60\n",
      "Step: 800 ,Average loss in 50 steps: 0.311 ,batch time: 0.00 ,Run time: 1.17 Batch Accuracy: 90.62 Valid accuracy: 89.80\n",
      "Step: 850 ,Average loss in 50 steps: 0.313 ,batch time: 0.02 ,Run time: 1.16 Batch Accuracy: 87.50 Valid accuracy: 86.60\n",
      "Step: 900 ,Average loss in 50 steps: 0.338 ,batch time: 0.00 ,Run time: 1.17 Batch Accuracy: 87.50 Valid accuracy: 87.20\n",
      "Step: 950 ,Average loss in 50 steps: 0.321 ,batch time: 0.01 ,Run time: 1.19 Batch Accuracy: 82.81 Valid accuracy: 85.60\n",
      "Step: 1000 ,Average loss in 50 steps: 0.331 ,batch time: 0.05 ,Run time: 1.11 Batch Accuracy: 85.94 Valid accuracy: 88.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 100/100 [01:35<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "num_steps=1001\n",
    "batch_size=64\n",
    "graph=tf.Graph()\n",
    "batch_timing=0\n",
    "run_timing=0\n",
    "restore_sess=1\n",
    "test_batch_size=int(num_comments_test/100)\n",
    "validation_batch_size=500\n",
    "#model_name=\"./ckpt/LR_word2vec.ckpt\"\n",
    "model_name=\"./ckpt/LR_word2vec_1M_FT_nopunc.ckpt\"\n",
    "#model_name=\"./ckpt/LR_word2vec_200k_Amazon.ckpt\"\n",
    "steps_display=min(int((num_steps-1)/20),1000)\n",
    "with graph.as_default():\n",
    "    inputs=tf.placeholder(tf.int32, shape=(batch_size,max(comment_lengths)))\n",
    "    labels=tf.placeholder(tf.float64, shape=(batch_size,1))\n",
    "    embedding_matrix=tf.placeholder(tf.float64, shape=(n_embeddings,d_embeddings))\n",
    "    test_inputs=tf.placeholder(tf.int32, shape=(test_batch_size,max(comment_lengths)))\n",
    "    validation_inputs=tf.placeholder(tf.int32, shape=(validation_batch_size,max(comment_lengths)))\n",
    "    W = tf.Variable(tf.random_uniform([comment_vector_size, 1],-1.0,1.0,dtype=tf.float64))\n",
    "    b = tf.Variable(tf.ones([1],dtype=tf.float64))\n",
    "    def model(data,batch_size=batch_size):\n",
    "        batch_embeddings=tf.reshape(tf.nn.embedding_lookup(embedding_matrix,data),[batch_size,max(comment_lengths)*d_embeddings])\n",
    "        y_matmul=tf.matmul(batch_embeddings,W)+b\n",
    "        return y_matmul\n",
    "    y_pred_train=model(inputs,batch_size=batch_size)\n",
    "    y_pred_test=model(test_inputs,batch_size=test_batch_size)\n",
    "    y_pred_validation=model(validation_inputs,batch_size=validation_batch_size)\n",
    "    test_predictions=tf.round(tf.sigmoid(y_pred_test))\n",
    "    train_predictions=tf.round(tf.sigmoid(y_pred_train)) \n",
    "    validation_predictions=tf.round(tf.sigmoid(y_pred_validation)) \n",
    "    x_entropy=tf.nn.sigmoid_cross_entropy_with_logits(logits=y_pred_train,labels=labels)\n",
    "    loss =tf.reduce_mean(x_entropy)#+0.01*(tf.reduce_sum(tf.multiply(W,W))+tf.reduce_sum(tf.multiply(b,b)))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver=tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    if restore_sess==1:\n",
    "        try:\n",
    "            saver.restore(sess,model_name)\n",
    "        except:\n",
    "            print('Unexpected Error: model cannot be restored')\n",
    "    print('initialized')\n",
    "    loss_val_sum=0\n",
    "    for i in range(num_steps):\n",
    "        time1=time.time()\n",
    "        batch_inputs,batch_labels,_=generate_batch(batch_size)\n",
    "        time2=time.time()\n",
    "        feed_dict={inputs:batch_inputs,labels:batch_labels,embedding_matrix:embeddings}\n",
    "        _,loss_val,train_preds=sess.run([optimizer,loss,train_predictions],feed_dict=feed_dict)\n",
    "        time3=time.time()\n",
    "        batch_timing += time2-time1\n",
    "        run_timing += time3-time2\n",
    "        loss_val_sum += loss_val\n",
    "        if i % steps_display==0 and i!=0:\n",
    "            validation_batch,validation_labels,_=generate_batch(validation_batch_size)\n",
    "            validation_preds=validation_predictions.eval({validation_inputs:validation_batch,embedding_matrix:embeddings})\n",
    "            print('Step:',i,\n",
    "                  ',Average loss in',steps_display, 'steps:',\"{0:.3f}\".format(loss_val_sum/steps_display),',batch time:'\n",
    "                  ,\"{0:.2f}\".format(batch_timing),',Run time:',\"{0:.2f}\".format(run_timing)\n",
    "                  ,'Batch Accuracy:', \"{:4.2f}\".format(100*accuracy_score(batch_labels,train_preds))\n",
    "                  ,'Valid accuracy:',\"{:4.2f}\".format(100*accuracy_score(validation_labels,validation_preds)))            \n",
    "            batch_timing=0\n",
    "            run_timing=0\n",
    "            loss_val_sum=0\n",
    "    y_pred=np.zeros((num_comments_test,1))\n",
    "    for i in tqdm(range(int(num_comments_test/test_batch_size))):\n",
    "        test_batch=test_dataset[i*test_batch_size:(i+1)*test_batch_size,:]\n",
    "        y_pred_batch=test_predictions.eval(feed_dict={test_inputs:test_batch,embedding_matrix:embeddings})\n",
    "        y_pred[i*test_batch_size:(i+1)*test_batch_size,:]=y_pred_batch            \n",
    "    Weights=W.eval()\n",
    "    bias=b.eval()\n",
    "    try:\n",
    "        saver.save(sess,model_name)\n",
    "        print('Model saved!')\n",
    "    except:\n",
    "        print('Model could not be saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.82%, Precision: 82.13%, Recall: 86.20%, F1_score: 86.73%\n"
     ]
    }
   ],
   "source": [
    "y_true=test_labels\n",
    "\n",
    "accuracy=accuracy_score(y_true,y_pred)\n",
    "precision=average_precision_score(y_true,y_pred)\n",
    "recall=recall_score(y_true,y_pred)\n",
    "f1=f1_score(y_true,y_pred)\n",
    "print('Accuracy: {:4.2f}%, Precision: {:4.2f}%, Recall: {:4.2f}%, F1_score: {:4.2f}%'.format(accuracy*100,precision*100,recall*100,f1*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
